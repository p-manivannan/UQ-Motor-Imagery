{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9515290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set notebook to use only one GPU\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce008765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datasets.moabb import MOABBDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from braindecode.preprocessing import create_windows_from_events\n",
    "from braindecode.preprocessing import (\n",
    "    exponential_moving_standardize, preprocess, Preprocessor)\n",
    "from numpy import multiply\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from models_bachelors import *\n",
    "from file_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b315848",
   "metadata": {},
   "source": [
    "# Preprocessing functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550ef50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    dataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids=None)\n",
    "    return dataset\n",
    "\n",
    "def preprocess_data(dataset):\n",
    "    # Parameters for exponential moving standardization\n",
    "    factor_new = 1e-3\n",
    "    init_block_size = 1000\n",
    "    # Factor to convert from V to mV\n",
    "    factor = 1e6\n",
    "\n",
    "    preprocessors = [\n",
    "        Preprocessor('pick_types', eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
    "        Preprocessor(lambda data: multiply(data, factor)),  # Convert from V to uV\n",
    "        Preprocessor(exponential_moving_standardize,  # Exponential moving standardization\n",
    "                    factor_new=factor_new, init_block_size=init_block_size)\n",
    "    ]\n",
    "\n",
    "    return preprocess(dataset, preprocessors)\n",
    "\n",
    "def epoch_data(dataset):\n",
    "    trial_start_offset_seconds = -0.5\n",
    "    # Extract sampling frequency, check that they are same in all datasets\n",
    "    sfreq = dataset.datasets[0].raw.info['sfreq']\n",
    "    assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
    "    # Calculate the trial start offset in samples.\n",
    "    trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
    "\n",
    "    # Create windows using braindecode function for this. It needs parameters to\n",
    "    # define how trials should be used.\n",
    "    windows_dataset = create_windows_from_events(\n",
    "        dataset,\n",
    "        trial_start_offset_samples=trial_start_offset_samples,\n",
    "        trial_stop_offset_samples=0,\n",
    "        preload=True,\n",
    "    )\n",
    "\n",
    "    return windows_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca37b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expects a BaseConcatDataset object\n",
    "# Iterate through subject datasets and create dataset of 9 rows and 576 columns\n",
    "# (9 subjects and 576 trials).\n",
    "def create_dataframe_helper(dataset):\n",
    "    subjects_lst = []\n",
    "    subjects_targets = []\n",
    "    for subject_id in range(0, len(dataset)):\n",
    "        # Append to list a set of inputs and targets from each run\n",
    "        # in subject dataset\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        subject_dataset = dataset[subject_id].datasets\n",
    "        for run in subject_dataset:\n",
    "            for trial in run:\n",
    "                inputs.append(trial[0])\n",
    "                targets.append(trial[1])\n",
    "        subjects_lst.append(inputs)\n",
    "        subjects_targets.append(targets)\n",
    "\n",
    "    return np.asarray(subjects_lst), np.asarray(subjects_targets)\n",
    "\n",
    "\n",
    "def create_dataframe(processed_data):\n",
    "    # Data to be saved gonna have shape (9, 576, 22, 1125)\n",
    "    # 9 subjects. 576 trials each. 22 channels. 1125 timestamps\n",
    "    split_data = processed_data.split('subject')\n",
    "    split_data = [split_data[str(i)] for i in range(1, 9 + 1)]\n",
    "    inputs, targets = create_dataframe_helper(split_data)\n",
    "    return inputs, targets\n",
    "\n",
    "def onehot(targets):\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    targets = targets.reshape(-1,1)\n",
    "    targets = encoder.fit_transform(targets)\n",
    "    n_subj = 9\n",
    "    n_trials = 576\n",
    "    n_classes = 4\n",
    "    targets = targets.reshape(n_subj, n_trials, n_classes)\n",
    "    return targets\n",
    "    \n",
    "\n",
    "def get_x_y(inputs, targets):\n",
    "    n_runs = inputs.shape[1] * inputs.shape[0]\n",
    "    channels = inputs.shape[2]\n",
    "    timestamps = inputs.shape[3]\n",
    "    n_classes = targets.shape[2]\n",
    "    X = np.vstack(inputs).reshape(n_runs, channels, timestamps)\n",
    "    Y = np.vstack(targets).reshape(n_runs, n_classes)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d19d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_data = preprocess_data(load_dataset())\n",
    "processed_data = epoch_data(pre_processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f25e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs, targets = create_dataframe(processed_data)\n",
    "targets = onehot(targets)\n",
    "print(inputs.shape, targets.shape)\n",
    "save_dict_to_hdf5({'inputs': inputs, 'targets': targets}, 'dataset.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae90ceb0",
   "metadata": {},
   "source": [
    "# Lockbox Creation\n",
    "\n",
    "Instead of saving a separate file containing the data of the lockboxed set, the lockbox file contains the indices of the to-be lockboxed set for each test subject. \n",
    "\n",
    "This way, the lockbox trials can be easily excluded during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0f3d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_bachelors import *\n",
    "from file_functions import *\n",
    "from numpy.random import default_rng\n",
    "from sklearn.model_selection import KFold\n",
    "loaded_inputs = inputs\n",
    "loaded_targets = targets\n",
    "\n",
    "# Create Lockbox for each subject\n",
    "n_s = 9  # Number of subjects\n",
    "\n",
    "'''\n",
    "NEED TO IGNORE THE TEST SUBJECT AND LOXCKBOX THE REST!\n",
    "YOU CAN USE KFOLD SPLIT FOR THISb  \n",
    "'''\n",
    "\n",
    "kfold_lock = KFold(n_splits= n_s, shuffle= False)\n",
    "rng = default_rng()\n",
    "\n",
    "lockbox = []\n",
    "\n",
    "'''\n",
    "MIGHT BE A PROBLEM THAT IM USING KFOLD TO SPLIT LOCKBOX HERE WHILE USING ANOTHER\n",
    "METHOD TO SELECT TRAIN AND TEST IDS ELSEWHERE.\n",
    "IT WOULD BE BEST IF I MAKE THE LOCKBOX A DICT OF DICTS OF NP ARRAY OF INDEXES (57,)\n",
    "'''\n",
    "\n",
    "# Split into train and test indices. The test indices are to indicate which\n",
    "# subject to save as and the train indices are where the magic happens\n",
    "for train_idx, test_idx in kfold_lock.split(loaded_inputs, loaded_targets):\n",
    "    lockbox_idx = []\n",
    "    # Perform the lockbox operation on the train indices as follows:\n",
    "    # Take 10% of the indices from each train subject, making sure \n",
    "    # to separate them by subject and NOT concatenating.\n",
    "    # This is because the indices are dependent on the subject.\n",
    "    for idx in train_idx:\n",
    "        subject_inputs = loaded_inputs[idx]\n",
    "        num_trials= subject_inputs.shape[0]\n",
    "        # Get random 10% of subject's trials\n",
    "        indexes = rng.choice(num_trials, size=int(0.1 * num_trials), replace=False)\n",
    "        lockbox_idx.append(indexes)\n",
    "\n",
    "    # This operation assumes index of lockbox corresponds to test_idx\n",
    "    lockbox.append(lockbox_idx)\n",
    "    \n",
    "\n",
    "lockbox = np.array(lockbox)\n",
    "save_dict_to_hdf5(dict({'data': lockbox}), 'lockbox')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
