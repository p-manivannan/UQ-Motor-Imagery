{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d13c46ea",
   "metadata": {},
   "source": [
    "# To-Do:\n",
    "- Get per subject AUROC scores and variance for each method.\n",
    "- Check if ensemble result analysis is handled properly: Shape of ensemble prediction set is (9, 10, 576, 4). This is the only case of such a shape and I don't know if it was handled well by the AUROC and accuracy calculating functions\n",
    "- Fix Flipout predictions: Issue is that I use X_test shape while predicting for lockbox which has a different shape. This affects the KL-weight parameter for flipout. Also, I don't know if this value is only fixed for the training set.\n",
    "- Tidy up code: Remove unneccesary files, make functions as general as possible, make a lot of documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f82b0ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "# Set notebook to use only one GPU\n",
    "%env CUDA_VISIBLE_DEVICES=1     \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da8eac33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keras Uncertainty will use standalone Keras backend"
     ]
    }
   ],
   "source": [
    "from result_analysis_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d765c1",
   "metadata": {},
   "source": [
    "# Accuracy and average uncertainty:\n",
    "Data shape of preds and lockbox: (9, 50, 576, 4), 50 forward passes. \n",
    "\n",
    "'''\n",
    "preds and lockbox shape: (9, 50, 576, 4).\n",
    "Axis -3 (50) should be collapsed only in the \n",
    "following cases:\n",
    "    - accuracy\n",
    "    - std. dev.\n",
    "Collapsing of the axis for uncertainty is in\n",
    "models_bachelors.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d901a1",
   "metadata": {},
   "source": [
    "### Accuracy of each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eacadb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CLEANUP IN PROGRESS\n",
    "'''\n",
    "\n",
    "acc_methods = {'mcdropconnect': {'test': [], 'lockbox': []},\n",
    "               'mcdropout': {'test': [], 'lockbox': []},\n",
    "               'flipout': {'test': [], 'lockbox': []},\n",
    "               'ensemble': {'test': [], 'lockbox': []},\n",
    "               'duq': {'test': [], 'lockbox': []},\n",
    "               'standard': {'test': [], 'lockbox': []},\n",
    "               'standard_dropconnect': {'test': [], 'lockbox': []}\n",
    "               }\n",
    "NUM = 50  # Number of prediction sets\n",
    "\n",
    "for method, dicts in acc_methods.items():\n",
    "    preds = load_predictions(method)\n",
    "# Load each prediction set and compute avg accuracy for each set, adding them to list\n",
    "for n in range(NUM):\n",
    "    methods = load_dict_from_hdf5(f'predictions/predictions_duq.h5')\n",
    "    for name, method in methods.items():\n",
    "        isStandard = True\n",
    "        # print(f'y_true: {method[\"test\"][\"labels\"].argmax(axis=-1).shape}, y_pred: {method[\"test\"][\"preds\"].argmax(axis=-1).shape}')\n",
    "        test_accs = get_accuracies(method[\"test\"], isStandard)   # Get test set accuracies\n",
    "        lock_accs = get_accuracies(method[\"lockbox\"], isStandard)\n",
    "        if name == 'mcdropconnect':\n",
    "            acc_mcdropconnect['test'].append(test_accs)\n",
    "            acc_mcdropconnect['lockbox'].append(lock_accs)\n",
    "        elif name == 'mcdropout':\n",
    "            acc_mcdropout['test'].append(test_accs)\n",
    "            acc_mcdropout['lockbox'].append(lock_accs)\n",
    "        elif name == 'standard':\n",
    "            acc_std['test'].append(test_accs)\n",
    "            acc_std['lockbox'].append(lock_accs)\n",
    "        elif name == 'standard_dropconnect':\n",
    "            acc_std_mcdropconnect['test'].append(test_accs)\n",
    "            acc_std_mcdropconnect['lockbox'].append(lock_accs)\n",
    "        elif name == 'flipout':\n",
    "            acc_flipout['test'].append(test_accs)\n",
    "            acc_flipout['lockbox'].append(lock_accs)\n",
    "        elif name == 'duq':\n",
    "            acc_duq['test'].append(test_accs)\n",
    "            acc_duq['lockbox'].append(lock_accs)\n",
    "        else:\n",
    "            acc_ensemble_dropout['test'].append(test_accs)\n",
    "            acc_ensemble_dropout['lockbox'].append(lock_accs)\n",
    "\n",
    "# Only for UQ:\n",
    "for name, key in acc_mcdropconnect.items():\n",
    "    key = np.array(key).mean(axis=0)\n",
    "\n",
    "for name, key in acc_mcdropout.items():\n",
    "    key = np.array(key).mean(axis=0)\n",
    "\n",
    "for name, key in acc_ensemble_dropout.items():\n",
    "    key = np.array(key).mean(axis=0)\n",
    "\n",
    "for name, key in acc_flipout.items():\n",
    "    key = np.array(key).mean(axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duq\n",
      "test set avg acc: 55.421 +/- 9.163\n",
      "lockbox set avg acc: 70.468 +/- 2.929\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy import round\n",
    "\n",
    "r = 5\n",
    "# print(round(np.std(acc_mcdropconnect[\"test\"]), 5))\n",
    "# print('mcdropout')\n",
    "# print(acc_mcdropout)\n",
    "# print(f'test set avg acc: {round(np.mean(acc_mcdropout[\"test\"]), r) * 100} +/- {round(np.std(acc_mcdropout[\"test\"]), r) * 100}')\n",
    "# print(f'lockbox set avg acc: {round(np.mean(acc_mcdropout[\"lockbox\"]), r) * 100} +/- {round(np.std(acc_mcdropout[\"lockbox\"]), r) * 100}\\n')\n",
    "\n",
    "# print('mcdropconnect')\n",
    "# print(f'test set avg acc: {round(np.mean(acc_mcdropconnect[\"test\"]), r) * 100} +/- {round(np.std(acc_mcdropconnect[\"test\"]), r) * 100}')\n",
    "# print(f'lockbox set avg acc: {round(np.mean(acc_mcdropconnect[\"lockbox\"]), r) * 100} +/- {round(np.std(acc_mcdropconnect[\"lockbox\"]), r) * 100}\\n')\n",
    "\n",
    "# print('standard_dropout')\n",
    "# print(f'test set avg acc: {round(np.mean(acc_std[\"test\"]), r) * 100} +/- {round(np.std(acc_std[\"test\"]), r) * 100}')\n",
    "# print(f'lockbox set avg acc: {round(np.mean(acc_std[\"lockbox\"]), r) * 100} +/- {round(np.std(acc_std[\"lockbox\"]), r) * 100}\\n')\n",
    "\n",
    "\n",
    "# print('standard_dropconnect')\n",
    "# print(f'test set avg acc: {round(np.mean(acc_std_mcdropconnect[\"test\"]), r) * 100} +/- {round(np.std(acc_std_mcdropconnect[\"test\"]), r) * 100}')\n",
    "# print(f'lockbox set avg acc: {round(np.mean(acc_std_mcdropconnect[\"lockbox\"]), r) * 100} +/- {round(np.std(acc_std_mcdropconnect[\"lockbox\"]), r) * 100}\\n')\n",
    "\n",
    "\n",
    "# print('ensemble_dropout')\n",
    "# print(f'test set avg acc: {round(np.mean(acc_ensemble_dropout[\"test\"]), r) * 100} +/- {round(np.std(acc_ensemble_dropout[\"test\"]), r) * 100}')\n",
    "# print(f'lockbox set avg acc: {round(np.mean(acc_ensemble_dropout[\"lockbox\"]), r) * 100} +/- {round(np.std(acc_ensemble_dropout[\"lockbox\"]), r) * 100}\\n')\n",
    "\n",
    "\n",
    "print('duq')\n",
    "print(f'test set avg acc: {round(np.mean(acc_duq[\"test\"]), r) * 100} +/- {round(np.std(acc_duq[\"test\"]), r) * 100}')\n",
    "print(f'lockbox set avg acc: {round(np.mean(acc_duq[\"lockbox\"]), r) * 100} +/- {round(np.std(acc_duq[\"lockbox\"]), r) * 100}\\n')\n",
    "\n",
    "# print('flipout')\n",
    "# print(f'test set avg acc: {round(np.mean(acc_flipout[\"test\"]), r) * 100} +/- {round(np.std(acc_flipout[\"test\"]), r) * 100}')\n",
    "# print(f'lockbox set avg acc: {round(np.mean(acc_flipout[\"lockbox\"]), r) * 100} +/- {round(np.std(acc_flipout[\"lockbox\"]), r) * 100}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, auc\n",
    "import matplotlib.patches as mpatches\n",
    "from numpy import round\n",
    "\n",
    "'''\n",
    "Gets a numpy array down to a 2D array\n",
    "'''\n",
    "def get_in_shape(data):\n",
    "    # If data has shape of 2 elements or less, assume it's already in shape\n",
    "    if len(data.shape) < 3:\n",
    "        return data\n",
    "    else:\n",
    "        while len(data.shape) > 2:\n",
    "            data = np.vstack(data)\n",
    "        return data\n",
    "\n",
    "\n",
    "'''\n",
    "data can be whatever shape.\n",
    "manual roc plot creation reqs:\n",
    "    y_pred: (9*50, 50, 576, 4)\n",
    "'''\n",
    "def make_roc_plot(y_true, y_pred, isStandard, unc_method):\n",
    "    '''\n",
    "    y_pred can be either of shape (50, 9, 50, 576, 4) or  (9, 50, 576, 4). We need it in shape (X, 4).\n",
    "    y_true can be either of shape (50, 9, 576, 4) or (9, 576, 4).\n",
    "    So apply same algorithm to get these sets into the shape (X, 4)\n",
    "    '''\n",
    "    thresholds = np.arange(0, 1.001, 0.001)\n",
    "    # print(f'y_true: {y_true.shape}, y_pred: {y_pred.shape}')\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    unc = get_uncertainty(y_pred, unc_method).flatten()\n",
    "    y_true = get_in_shape(y_true)\n",
    "    y_pred = get_in_shape(y_pred) if isStandard else get_in_shape(y_pred.mean(axis=-3))\n",
    "    \n",
    "    # print(f'y_true: {y_true.shape}, y_pred: {y_pred.shape}, certains: {unc.shape}')\n",
    "    for t in thresholds:\n",
    "        '''\n",
    "        Order is reversed because FPR and TPR are reversed for some reason\n",
    "        '''\n",
    "        certains = (t < unc)           # Certain when uncertainty is below threshold\n",
    "        uncertains = (t > unc)\n",
    "        # Calculate TPR and FPR\n",
    "        tp = sum(y_pred.argmax(axis=1)[uncertains] != y_true.argmax(axis=1)[uncertains])     # N. preds uncertain predictions that are incorrect\n",
    "        fn = sum(y_pred.argmax(axis=1)[certains] != y_true.argmax(axis=1)[certains])      # Prediction that's certain and incorrect\n",
    "        fp = sum(y_pred.argmax(axis=1)[uncertains] == y_true.argmax(axis=1)[uncertains])   # prediction that's uncertain and correct\n",
    "        tn = sum(y_pred.argmax(axis=1)[certains] == y_true.argmax(axis=1)[certains]) # Prediction that's certain and correct\n",
    "        # print(f'tp: {tp} fn: {fn} fp: {fp} tn: {tn}')\n",
    "        fpr.append(fp / (fp + tn))\n",
    "        tpr.append(tp / (tp + fn))\n",
    "    # fig1, ax1 = plt.subplots()\n",
    "    # hist_correct, bins_correct, _ = ax1.hist(auc, bins=10, density=False, alpha=0.5, label='Correct')\n",
    "    # fig1.show()\n",
    "    return np.array(tpr), np.array(fpr)\n",
    "\n",
    "\n",
    "'''\n",
    "I calculate AUROC and plot ROC separately because I want to get\n",
    "mean AUROC of all 50 prediction sets along with their variance.\n",
    "Then I plot ROC with all 50 prediction sets.\n",
    "'''\n",
    "def roc_plot_and_auroc(method, key, unc_method):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    aucs_lst = []\n",
    "    # isStandard = True if 'standard' in method else False\n",
    "    isStandard = False\n",
    "    # num_predictions = 50 if not isStandard else 1\n",
    "    num_predictions = 1\n",
    "    # creation of set of 50 predictions, as well as AUROC score calculation\n",
    "    for n in range(num_predictions):\n",
    "        # methods = load_predictions(n, 'duq')\n",
    "        methods = load_dict_from_hdf5(f'predictions/predictions_ensemble_dropout.h5')\n",
    "        data = methods[method][key]\n",
    "        tpr, fpr = make_roc_plot(data['labels'], data['preds'], isStandard, unc_method)\n",
    "        # print(f'y_true shape: {y_true_roc.shape} y_pred: {y_pred_roc.shape}')\n",
    "        auroc_score = auc(tpr, fpr)\n",
    "        aucs_lst.append(auroc_score)\n",
    "        y_pred.append(data['preds'])\n",
    "        y_true.append(data['labels'])\n",
    "\n",
    "    tpr, fpr = make_roc_plot(np.vstack(y_true), np.vstack(y_pred), isStandard, unc_method)\n",
    "\n",
    "    return tpr, fpr, aucs_lst\n",
    "\n",
    "\n",
    "# aucs_test = {'predictive-entropy':\n",
    "#           {'mcdropconnect':[], 'mcdropout':[], 'standard':[], 'standard_dropconnect':[]},\n",
    "#           'shannon-entropy':\n",
    "#           {'mcdropconnect':[], 'mcdropout':[], 'standard':[], 'standard_dropconnect':[]},\n",
    "#           'mutual-information':\n",
    "#           {'mcdropconnect':[], 'mcdropout':[], 'standard':[], 'standard_dropconnect':[]}\n",
    "#         }\n",
    "# aucs_test = {'predictive-entropy': {'mcdropconnect':[], 'mcdropout':[], 'standard':[], 'standard_dropconnect':[]}}\n",
    "# methods = {'mcdropconnect':'C0', 'mcdropout':'C1', 'standard':'C4', 'standard_dropconnect':'C9'}\n",
    "# aucs_test = {'predictive-entropy': {'ensemble_dropout': []}}\n",
    "# aucs_test = {'predictive-entropy': {'duq': []}}\n",
    "aucs_test = {'predictive-entropy': {'ensemble_dropout': []}, 'shannon-entropy': {'ensemble_dropout': []}, 'mutual-information': {'ensemble_dropout': []}}\n",
    "# fig1, ax1 = plt.subplots(2, squeeze=False, figsize=(10, 20))\n",
    "# ax1 = ax1.flatten()\n",
    "key = \"test\"\n",
    "print(key)\n",
    "for unc_name, methods_dict in aucs_test.items():\n",
    "    print(unc_name)\n",
    "    for method, auc_lst in methods_dict.items():\n",
    "        tpr, fpr, ret_aucs = roc_plot_and_auroc(method, key, unc_name)\n",
    "        print(f'{key} AUC: {round(np.mean(ret_aucs), r) * 100} +/- {round(np.std(ret_aucs), r) * 100}')\n",
    "        # Normalizes AREA UNDER CURVE to sum up to 1. y-axis values are meaningless.\n",
    "        print(method)\n",
    "        r = 6\n",
    "        # ax1[0].plot(tpr, fpr, color=methods[method])\n",
    "\n",
    "key = \"lockbox\"\n",
    "print(key)\n",
    "for unc_name, methods_dict in aucs_test.items():\n",
    "    print(unc_name)\n",
    "    for method, auc_lst in methods_dict.items():\n",
    "        r = 6\n",
    "        tpr, fpr, ret_aucs = roc_plot_and_auroc(method, key, unc_name)\n",
    "        print(f'{key} AUC: {round(np.mean(ret_aucs), r) * 100} +/- {round(np.std(ret_aucs), r) * 100}')\n",
    "        # Normalizes AREA UNDER CURVE to sum up to 1. y-axis values are meaningless.\n",
    "        print(method)\n",
    "        # ax1[1].plot(tpr, fpr, color=methods[method])\n",
    "\n",
    "# red_patch = mpatches.Patch(color=methods['mcdropconnect'], label='MC-DropConnect')\n",
    "# green_patch = mpatches.Patch(color=methods['mcdropout'], label='MC-Dropout')\n",
    "# blue_patch = mpatches.Patch(color=methods['standard'], label='Standard Dropout')\n",
    "# yellow_patch = mpatches.Patch(color=methods['standard_dropconnect'], label='Standard Dropconnect')\n",
    "\n",
    "# ax1[0].legend(handles=[red_patch, green_patch, blue_patch, yellow_patch], fontsize=20)\n",
    "# ax1[0].set_title(f'Out-of-population ROC', fontsize=35)\n",
    "# ax1[0].set_xlabel(\"FPR\", fontsize=30)\n",
    "# ax1[0].set_ylabel(\"TPR\", fontsize=30)\n",
    "\n",
    "# ax1[1].legend(handles=[red_patch, green_patch, blue_patch, yellow_patch], fontsize=20)\n",
    "# ax1[1].set_title(f'Within-population ROC', fontsize=35)\n",
    "# ax1[1].set_xlabel(\"FPR\", fontsize=30)\n",
    "# ax1[1].set_ylabel(\"TPR\", fontsize=30)\n",
    "\n",
    "# # plt.rcParams.update({'font.size': 22})\n",
    "# # fig1.savefig(f'roc_plot_pred_entropy_w_10_h_20.pdf')\n",
    "# ax1[0].tick_params(axis='x', labelsize=20)\n",
    "# ax1[0].tick_params(axis='y', labelsize=20)\n",
    "# ax1[1].tick_params(axis='x', labelsize=20)\n",
    "# ax1[1].tick_params(axis='y', labelsize=20)\n",
    "\n",
    "# fig1.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88310a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1.savefig(f'roc_plot_pred_entropy_w_10_h_20_large.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if there is a significant difference between the AUROCs of two methods with a certain metric on one set, need to construct and save a MATLAB struct with two elements:\n",
    "\n",
    "- spsizes: 2 x 1 vector with samples sizes for X and Y, uncertainties and targets\n",
    "- ratings: K x N matrix. K is uncertainties/targets of each model, it is the row. N is the sum of len(X) and len(Y) and first len(X) elements are uncertainties and last len(Y) elements are targets\n",
    "\n",
    "Then I gotta save this data as a MATLAB struct while in python somehow\n",
    "\n",
    "Things I want to compare for test and lockbox:\n",
    "- MC-Dropout: mutual information and shannon entropy\n",
    "- MC-DropConnect: mutual information and shannon entropy\n",
    "\n",
    "So 2 separate files to save, one for test and the other for lockbox\n",
    "\n",
    "y_true/ground truth labels will be 0, 1. And will be 1 when y_true != y_pred and 0 when it isn't. So basically, 1 for all incorrects (which should be uncertains) and 0 for all corrects (which should be certains)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per subject scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Per-subject uncertainties and AUROC\n",
    "This is exactly what I need:\n",
    "    - Per subject AUROC. This can only be done with array of shape (9, 576).\n",
    "        - start w/ (50, 9, 50, 576, 4) for a method.\n",
    "        - Mean axis=0 -> (9, 50, 576, 4)\n",
    "        - Get uncertainties -> (9, 576) -> Mean axis -1=Avg. uncertainties -> (9, 1)\n",
    "        - For each subject in axis 0, calculate AUROC to get final array of (9, 1)\n",
    "    - Array of shape (9, 1) for uncertanties\n",
    "        - Get uncertainties -> (9, 576) -> Mean axis -1=Avg. uncertainties -> (9, 1)\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.patches as mpatches\n",
    "import scipy.stats as stats\n",
    "from numpy import round\n",
    "\n",
    "def load_all_predictions():\n",
    "    predictions = {'mcdropout': \n",
    "            {'test': {'preds':[], 'labels':[]}, \n",
    "                'lockbox':{'preds':[], 'labels':[]}},\n",
    "                'mcdropconnect': \n",
    "            {'test': {'preds':[], 'labels':[]}, \n",
    "                'lockbox':{'preds':[], 'labels':[]}},\n",
    "                'standard': \n",
    "            {'test': {'preds':[], 'labels':[]}, \n",
    "                'lockbox':{'preds':[], 'labels':[]}},\n",
    "                'standard_dropconnect': \n",
    "            {'test': {'preds':[], 'labels':[]}, \n",
    "                'lockbox':{'preds':[], 'labels':[]}}\n",
    "            }\n",
    "    # Load all UQ method predictions\n",
    "    N = 50\n",
    "    for n in range(N):\n",
    "        dataset = load_predictions(n, False)\n",
    "        # No need to append the labels because they're the same each time.\n",
    "        # So only need to append them once.\n",
    "        predictions['mcdropout']['test']['preds'].append(dataset['mcdropout']['test']['preds'])\n",
    "        predictions['mcdropout']['lockbox']['preds'].append(dataset['mcdropout']['lockbox']['preds'])\n",
    "        predictions['mcdropconnect']['test']['preds'].append(dataset['mcdropconnect']['test']['preds'])\n",
    "        predictions['mcdropconnect']['lockbox']['preds'].append(dataset['mcdropconnect']['lockbox']['preds'])\n",
    "\n",
    "    # Load all Standard method predictions\n",
    "    dataset = load_predictions(1, True)\n",
    "    predictions['standard']['test']['preds'].append(dataset['standard']['test']['preds'])\n",
    "    predictions['standard']['lockbox']['preds'].append(dataset['standard']['lockbox']['preds'])\n",
    "    predictions['standard_dropconnect']['test']['preds'].append(dataset['standard_dropconnect']['test']['preds'])\n",
    "    predictions['standard_dropconnect']['lockbox']['preds'].append(dataset['standard_dropconnect']['lockbox']['preds'])\n",
    "\n",
    "    # Load all the labels once into their respective method dicts\n",
    "    for name, method in predictions.items():\n",
    "        # standard labels are same as UQ method labels.\n",
    "        method['test']['labels'] = dataset['standard']['test']['labels']\n",
    "        method['lockbox']['labels'] = dataset['standard']['lockbox']['labels']\n",
    "        # convert relevant lists to numpy arrays\n",
    "        method['test']['preds'] = np.array(method['test']['preds'])\n",
    "        method['lockbox']['preds'] = np.array(method['lockbox']['preds'])\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "'''\n",
    "data can be whatever shape.\n",
    "manual roc plot creation reqs:\n",
    "    y_pred: (50, 576, 4)\n",
    "'''\n",
    "def get_fpr_tpr(y_true, y_pred, unc, isStandard):\n",
    "    '''\n",
    "    y_pred can be either of shape (50, 9, 50, 576, 4) or  (9, 50, 576, 4). We need it in shape (X, 4).\n",
    "    y_true can be either of shape (50, 9, 576, 4) or (9, 576, 4).\n",
    "    So apply same algorithm to get these sets into the shape (X, 4)\n",
    "    '''\n",
    "    thresholds = np.arange(0, 1.001, 0.001)\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    y_pred = y_pred if isStandard else y_pred.mean(axis=-3)\n",
    "    \n",
    "    # print(f'y_true: {y_true.shape}, y_pred: {y_pred.shape}, certains: {unc.shape}')\n",
    "    for t in thresholds:\n",
    "        '''\n",
    "        Order is reversed because FPR and TPR are reversed for some reason\n",
    "        '''\n",
    "        certains = (t < unc)           # Certain when uncertainty is below threshold\n",
    "        uncertains = (t > unc)\n",
    "        # Calculate TPR and FPR\n",
    "        tp = sum(y_pred.argmax(axis=1)[uncertains] != y_true.argmax(axis=1)[uncertains])     # N. preds uncertain predictions that are incorrect\n",
    "        fn = sum(y_pred.argmax(axis=1)[certains] != y_true.argmax(axis=1)[certains])      # Prediction that's certain and incorrect\n",
    "        fp = sum(y_pred.argmax(axis=1)[uncertains] == y_true.argmax(axis=1)[uncertains])   # prediction that's uncertain and correct\n",
    "        tn = sum(y_pred.argmax(axis=1)[certains] == y_true.argmax(axis=1)[certains]) # Prediction that's certain and correct\n",
    "        # print(f'tp: {tp} fn: {fn} fp: {fp} tn: {tn}')\n",
    "        fpr.append(fp / (fp + tn))\n",
    "        tpr.append(tp / (tp + fn))\n",
    "    return np.array(tpr), np.array(fpr)\n",
    "\n",
    "def get_auroc(y_true, y_pred, unc, isStandard):\n",
    "    tpr, fpr = get_fpr_tpr(y_true, y_pred, unc, isStandard)\n",
    "    return auc(tpr, fpr)\n",
    "\n",
    "def per_subject_metrics(data, isStandard):\n",
    "    key_set = data[key]        # Whether lockbox or preds of the method\n",
    "    y_true = key_set['labels']\n",
    "    y_preds = key_set['preds'].mean(axis=0)     # CHANGE: FIRST AXIS IS NOT ALWAYS 50! FOR METHODS WITHOUT 50 SETS IT'S SIMPLY 9!\n",
    "    unc = predictive_uncertainty(y_preds, 'predictive-entropy')\n",
    "    per_subject_aucs = []\n",
    "    # print(y_preds.shape)\n",
    "    for subject_id in range(y_preds.shape[0]):\n",
    "        per_subject_aucs.append(get_auroc(y_true[subject_id], y_preds[subject_id], unc[subject_id], isStandard))\n",
    "\n",
    "    \n",
    "    return np.array(per_subject_aucs), unc.mean(axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "'''\n",
    "data: (50, 9, 50, 576, 4)\n",
    "method: 'mcdropconnect'/'mcdropout'/'standard'/'standard_dropconnect'\n",
    "key: 'test'/'lockbox'\n",
    "'''\n",
    "def do_everything(data, method, key):\n",
    "    # data shape for UQ preds: (50, 9, 50, 576, 4)\n",
    "    isStandard = True if 'standard' in method else False\n",
    "    aurocs, uncertainties = per_subject_metrics(data, isStandard)\n",
    "    return aurocs, uncertainties\n",
    "\n",
    "\n",
    "aucs = {'mcdropconnect':[], 'mcdropout':[], 'standard':[], 'standard_dropconnect':[]}\n",
    "uncertainties = {'mcdropconnect':[], 'mcdropout':[], 'std':[], 'standard_dropconnect':[]}\n",
    "methods = {'mcdropconnect':'red', 'mcdropout':'green', 'standard':'blue', 'standard_dropconnect':'yellow'}\n",
    "fig1, ax1 = plt.subplots()\n",
    "key = \"test\"\n",
    "data = load_all_predictions()       # all prediction sets across all methods into a single object\n",
    "for method, values in data.items():\n",
    "    aurocs, uncs = do_everything(values, method, key)\n",
    "    # Normalizes AREA UNDER CURVE to sum up to 1. y-axis values are meaningless.\n",
    "    print(method)\n",
    "    r = 6\n",
    "    aucs[method] = aurocs\n",
    "    uncertainties[method] = uncs\n",
    "    # print(f'{key} set avg AUROC: {np.mean(aucs)} +/- {np.std(aucs)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method, values in aucs.items():\n",
    "    print(f'\\n\\n{method}\\n aucs:{values}\\n uncs: {uncertainties[method]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
